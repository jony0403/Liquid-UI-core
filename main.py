# main.py
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
import google.generativeai as genai
import httpx
from bs4 import BeautifulSoup
import asyncio
import os
from collections import OrderedDict # [NEW] ìºì‹±ì„ ìœ„í•œ ë„êµ¬
from dotenv import load_dotenv
load_dotenv()

# ==========================================
# [API KEY] ë„¤ í‚¤ê°€ ì—¬ê¸° ìˆì–´ì•¼ í•œë‹¤.
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
# ==========================================

genai.configure(api_key=GOOGLE_API_KEY)
model = genai.GenerativeModel('gemini-2.5-flash')

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# [NEW] 1. ìºì‹œ ì €ì¥ì†Œ í´ë˜ìŠ¤ ì •ì˜ (ë‡Œì˜ ë‹¨ê¸° ê¸°ì–µì¥ì¹˜)
class LocalCache:
    def __init__(self, capacity: int = 100):
        self.cache = OrderedDict()
        self.capacity = capacity # ìµœëŒ€ 100ê°œê¹Œì§€ë§Œ ê¸°ì–µ

    def get(self, key: str):
        if key not in self.cache:
            return None
        self.cache.move_to_end(key) # ìµœê·¼ì— ì¼ìœ¼ë‹ˆ ë§¨ ë’¤ë¡œ
        return self.cache[key]

    def put(self, key: str, value: str):
        if key in self.cache:
            self.cache.move_to_end(key)
        self.cache[key] = value
        if len(self.cache) > self.capacity:
            self.cache.popitem(last=False) # ì˜¤ë˜ëœ ê¸°ì–µ ì‚­ì œ

# ì„œë²„ê°€ ì¼œì§€ë©´ ë¹ˆ ê¸°ì–µì¥ì¹˜ ìƒì„±
summary_cache = LocalCache()

class AnalyzeRequest(BaseModel):
    url: str
    text_content: str | None = None

# [Final Version] ì‚°íƒ„ì´ ë°©ì‹ í¬ë¡¤ëŸ¬ (ëª¨ë“  íƒœê·¸ ë‹¤ ë’¤ì§)
async def fetch_page_content(url: str):
    DEFAULT_IMAGE = "https://images.unsplash.com/photo-1504711434969-e33886168f5c?q=80&w=3870&auto=format&fit=crop"

    # [í•µì‹¬ 1] í—¤ë”ë¥¼ ì§„ì§œ ì‚¬ëŒì²˜ëŸ¼ ì™„ë²½í•˜ê²Œ ìœ„ì¥
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
        "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
        "Referer": "https://www.naver.com/" 
    }
    
    async with httpx.AsyncClient(follow_redirects=True) as client:
        try:
            response = await client.get(url, headers=headers, timeout=10.0)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # 1. í…ìŠ¤íŠ¸ ì¶”ì¶œ
            for script in soup(["script", "style", "nav", "footer", "header", "iframe", "button"]):
                script.decompose()
            text = soup.get_text(separator=' ', strip=True)[:5000]
            
            # 2. ì´ë¯¸ì§€ ì¶”ì¶œ (ìš°ì„ ìˆœìœ„ë³„ë¡œ ìƒ…ìƒ…ì´ ë’¤ì§)
            image_url = ""
            
            # [Level 1] ë©”íƒ€ íƒœê·¸ (ê°€ì¥ í™•ì‹¤í•¨)
            candidates = [
                soup.find("meta", property="og:image"),
                soup.find("meta", name="twitter:image"),
                soup.find("meta", property="twitter:image") # ê°€ë” propertyë¡œ ì“°ëŠ” ì• ë“¤ë„ ìˆìŒ
            ]
            
            for candidate in candidates:
                if candidate and candidate.get("content"):
                    image_url = candidate["content"]
                    print(f"âœ… ë©”íƒ€ íƒœê·¸ì—ì„œ ì´ë¯¸ì§€ í™•ë³´: {image_url[:30]}...")
                    break
            
            # [Level 2] ë³¸ë¬¸ ì´ë¯¸ì§€ ê°•ì œ ìˆ˜ìƒ‰ (ë©”íƒ€ íƒœê·¸ê°€ ì—†ì„ ë•Œ)
            if not image_url:
                # ë„¤ì´ë²„ ë‰´ìŠ¤, ì—°ì˜ˆ, ìŠ¤í¬ì¸ , í¬ìŠ¤íŠ¸ ë“± ì˜¨ê°– ID/Class ì´ì§‘í•©
                selectors = [
                    "#img1", # ì—°ì˜ˆë‰´ìŠ¤ ëŒ€í‘œ ì´ë¯¸ì§€
                    ".end_photo_org img", # ì¼ë°˜ë‰´ìŠ¤ ë³¸ë¬¸ ì´ë¯¸ì§€
                    "#articleBodyContents img", 
                    "#newsEndContents img",
                    ".sc_view_img", # í¬ìŠ¤íŠ¸/ë¸”ë¡œê·¸
                    "figure img",   # ì¼ë°˜ì ì¸ HTML5 êµ¬ì¡°
                    ".media_end_head_photo_img" # ìµœì‹  ë„¤ì´ë²„ ë‰´ìŠ¤ í—¤ë”
                ]
                
                for selector in selectors:
                    img_tag = soup.select_one(selector)
                    if img_tag and img_tag.get("src"):
                        image_url = img_tag["src"]
                        print(f"âœ… ë³¸ë¬¸ íƒœê·¸({selector})ì—ì„œ ì´ë¯¸ì§€ í™•ë³´: {image_url[:30]}...")
                        break

            # [ê²°ê³¼ íŒì •]
            if not image_url:
                print("âš ï¸ ëª¨ë“  ìˆ˜ìƒ‰ ì‹¤íŒ¨. ê¸°ë³¸ ì´ë¯¸ì§€ ì‚¬ìš©.")
                image_url = DEFAULT_IMAGE
            
            return text, image_url
            
        except Exception as e:
            print(f"Crawling Error: {e}")
            return None, None
    DEFAULT_IMAGE = "https://images.unsplash.com/photo-1504711434969-e33886168f5c?q=80&w=3870&auto=format&fit=crop"

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }
    
    async with httpx.AsyncClient(follow_redirects=True) as client:
        try:
            response = await client.get(url, headers=headers, timeout=10.0)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # 1. í…ìŠ¤íŠ¸ ì¶”ì¶œ (ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°)
            for script in soup(["script", "style", "nav", "footer", "header", "iframe"]):
                script.decompose()
            text = soup.get_text(separator=' ', strip=True)[:5000]
            
            # 2. ì´ë¯¸ì§€ ì¶”ì¶œ (3ì¤‘ ì•ˆì „ì¥ì¹˜)
            image_url = ""
            
            # [ì‹œë„ 1] ë©”íƒ€ íƒœê·¸ (og:image) - ê°€ì¥ í™”ì§ˆ ì¢‹ìŒ
            og_image = soup.find("meta", property="og:image")
            if og_image and og_image.get("content"):
                image_url = og_image["content"]
            
            # [ì‹œë„ 2] íŠ¸ìœ„í„° íƒœê·¸ (twitter:image) - og:image ì—†ì„ ë•Œ
            if not image_url:
                tw_image = soup.find("meta", name="twitter:image")
                if tw_image and tw_image.get("content"):
                    image_url = tw_image["content"]
            
            # [ì‹œë„ 3] ë³¸ë¬¸ ì•ˆì—ì„œ ì²« ë²ˆì§¸ ì´ë¯¸ì§€ ì°¾ê¸° (ë„¤ì´ë²„ ë‰´ìŠ¤ íŠ¹í™”)
            if not image_url:
                # ë„¤ì´ë²„ ë‰´ìŠ¤ ë³¸ë¬¸ ì˜ì—­ IDë“¤ (#dic_area: ì¼ë°˜ë‰´ìŠ¤, #articeBody: ì—°ì˜ˆ ë“±)
                content_body = soup.select_one("#dic_area, #articleBodyContents, .news_end, #newsEndContents")
                if content_body:
                    first_img = content_body.find("img")
                    if first_img and first_img.get("src"):
                        image_url = first_img["src"]

            # [ê²°ê³¼ íŒì •]
            if image_url:
                print(f"ğŸ“¸ ì´ë¯¸ì§€ ë°œê²¬ ì„±ê³µ: {image_url[:50]}...")
            else:
                print("âš ï¸ ëë‚´ ì´ë¯¸ì§€ë¥¼ ëª» ì°¾ì•˜ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì´ë¯¸ì§€ ì‚¬ìš©.")
                image_url = DEFAULT_IMAGE
            
            return text, image_url
            
        except Exception as e:
            print(f"Crawling Error: {e}")
            return None, None
        
    # ìš°ë¦¬ê°€ ì‚¬ìš©í•  'ê¸°ë³¸ ì´ë¯¸ì§€' (ì´ë¯¸ì§€ ëª» ì°¾ì•˜ì„ ë•Œ ë„ìš¸ ì§¤)
    DEFAULT_IMAGE = "https://images.unsplash.com/photo-1504711434969-e33886168f5c?q=80&w=3870&auto=format&fit=crop"

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }
    async with httpx.AsyncClient(follow_redirects=True) as client:
        try:
            response = await client.get(url, headers=headers, timeout=10.0)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # 1. í…ìŠ¤íŠ¸ ì¶”ì¶œ
            for script in soup(["script", "style", "nav", "footer", "header"]):
                script.decompose()
            text = soup.get_text(separator=' ', strip=True)[:5000]
            
            # 2. ì´ë¯¸ì§€ ì¶”ì¶œ (og:image -> twitter:image -> ì—†ìœ¼ë©´ ê¸°ë³¸ê°’)
            image_url = ""
            og_image = soup.find("meta", property="og:image")
            
            if og_image and og_image.get("content"):
                image_url = og_image["content"]
            else:
                # [í•µì‹¬] ì´ë¯¸ì§€ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ì´ë¯¸ì§€ë¥¼ ë„£ì–´ë¼!
                print("âš ï¸ ì´ë¯¸ì§€ë¥¼ ëª» ì°¾ì•˜ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                image_url = DEFAULT_IMAGE
            
            return text, image_url
            
        except Exception as e:
            print(f"Crawling Error: {e}")
            return None, None
        
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }
    async with httpx.AsyncClient(follow_redirects=True) as client:
        try:
            response = await client.get(url, headers=headers, timeout=10.0)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            
            for script in soup(["script", "style", "nav", "footer", "header"]):
                script.decompose()
            text = soup.get_text(separator=' ', strip=True)[:5000]
            
            image_url = ""
            og_image = soup.find("meta", property="og:image")
            if og_image:
                image_url = og_image["content"]
            
            return text, image_url
        except Exception as e:
            print(f"Crawling Error: {e}")
            return None, None

# [Update] ìºì‹œ ì €ì¥ì„ ìœ„í•´ êµ¬ì¡° ë³€ê²½
async def gemini_stream_generator(text, image_url, url_key):
    full_response = "" # ì „ì²´ ë‚´ìš©ì„ ëª¨ì„ ë³€ìˆ˜
    
    # 1. ì´ë¯¸ì§€ URL ë¨¼ì € ì „ì†¡
    if image_url:
        img_msg = f"IMAGE_URL::{image_url}::END\n"
        yield img_msg
        full_response += img_msg
    
    prompt = f"""
    [System Instruction]
    ë„ˆëŠ” 'Liquid UI'ì˜ AI ì—”ì§„ì´ë‹¤.
    ì‚¬ìš©ìê°€ ì œê³µí•œ ì›¹í˜ì´ì§€ í…ìŠ¤íŠ¸ë¥¼ ëª¨ë°”ì¼ í™˜ê²½ì— ë§ì¶° [3ì¤„ ìš”ì•½]í•´ë¼.
    í•µì‹¬ ì •ë³´ë§Œ ë‚¨ê¸°ê³ , ë§íˆ¬ëŠ” ê±´ì¡°í•˜ê³  ëª…í™•í•˜ê²Œ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ë¼.
    
    [Input Text]
    {text} 
    """

    try:
        response = await model.generate_content_async(prompt, stream=True)
        async for chunk in response:
            if chunk.text:
                yield chunk.text
                full_response += chunk.text # ë§í•˜ëŠ” ì¡±ì¡± ëª¨ì€ë‹¤
        
        # [NEW] 2. ë‹¤ ë§í–ˆìœ¼ë©´ ìºì‹œì— ì €ì¥ (URLì„ í‚¤ê°’ìœ¼ë¡œ)
        summary_cache.put(url_key, full_response)
        print(f"ğŸ’¾ Cached Saved for: {url_key}")

    except Exception as e:
        yield f"Error: {str(e)}"

@app.post("/analyze")
async def analyze_page(request: AnalyzeRequest):
    print(f"Request received for: {request.url}")
    
    # [NEW] 3. ìºì‹œ í™•ì¸ (ê¸°ì–µ ì†ì— ìˆë‚˜?)
    cached_data = summary_cache.get(request.url)
    if cached_data:
        print(f"âš¡ Cache Hit! (ì´ˆê³ ì† ì‘ë‹µ): {request.url}")
        # ì €ì¥ëœ ê±° ë°”ë¡œ ë±‰ì–´ì£¼ëŠ” í•¨ìˆ˜
        async def cached_stream():
            yield cached_data
        return StreamingResponse(cached_stream(), media_type="text/event-stream")

    # ìºì‹œì— ì—†ìœ¼ë©´? -> í¬ë¡¤ë§ ì‹œì‘
    target_text = request.text_content
    image_url = ""

    if not target_text:
        print("Text missing. Server will crawl...")
        fetched_text, fetched_image = await fetch_page_content(request.url)
        if fetched_text:
            target_text = fetched_text
            image_url = fetched_image
        else:
            return StreamingResponse(iter(["Error: í¬ë¡¤ë§ ì‹¤íŒ¨"]), media_type="text/event-stream")
    
    # AI ìƒì„± ì‹œì‘ (urlë„ ê°™ì´ ë„˜ê²¨ì„œ ë‚˜ì¤‘ì— ì €ì¥í•˜ê²Œ í•¨)
    return StreamingResponse(
        gemini_stream_generator(target_text, image_url, request.url), 
        media_type="text/event-stream"
    )

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)