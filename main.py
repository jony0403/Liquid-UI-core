# main.py
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
import google.generativeai as genai
import httpx
from bs4 import BeautifulSoup
import asyncio
import os
from collections import OrderedDict # [NEW] ìºì‹±ì„ ìœ„í•œ ë„êµ¬
from dotenv import load_dotenv
load_dotenv()

# ==========================================
# [API KEY] ë„¤ í‚¤ê°€ ì—¬ê¸° ìˆì–´ì•¼ í•œë‹¤.
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
# ==========================================

genai.configure(api_key=GOOGLE_API_KEY)
model = genai.GenerativeModel('gemini-2.5-flash')

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# [NEW] 1. ìºì‹œ ì €ì¥ì†Œ í´ë˜ìŠ¤ ì •ì˜ (ë‡Œì˜ ë‹¨ê¸° ê¸°ì–µì¥ì¹˜)
class LocalCache:
    def __init__(self, capacity: int = 100):
        self.cache = OrderedDict()
        self.capacity = capacity # ìµœëŒ€ 100ê°œê¹Œì§€ë§Œ ê¸°ì–µ

    def get(self, key: str):
        if key not in self.cache:
            return None
        self.cache.move_to_end(key) # ìµœê·¼ì— ì¼ìœ¼ë‹ˆ ë§¨ ë’¤ë¡œ
        return self.cache[key]

    def put(self, key: str, value: str):
        if key in self.cache:
            self.cache.move_to_end(key)
        self.cache[key] = value
        if len(self.cache) > self.capacity:
            self.cache.popitem(last=False) # ì˜¤ë˜ëœ ê¸°ì–µ ì‚­ì œ

# ì„œë²„ê°€ ì¼œì§€ë©´ ë¹ˆ ê¸°ì–µì¥ì¹˜ ìƒì„±
summary_cache = LocalCache()

class AnalyzeRequest(BaseModel):
    url: str
    text_content: str | None = None

async def fetch_page_content(url: str):
    # ìš°ë¦¬ê°€ ì‚¬ìš©í•  'ê¸°ë³¸ ì´ë¯¸ì§€' (ì´ë¯¸ì§€ ëª» ì°¾ì•˜ì„ ë•Œ ë„ìš¸ ì§¤)
    DEFAULT_IMAGE = "https://images.unsplash.com/photo-1504711434969-e33886168f5c?q=80&w=3870&auto=format&fit=crop"

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }
    async with httpx.AsyncClient(follow_redirects=True) as client:
        try:
            response = await client.get(url, headers=headers, timeout=10.0)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # 1. í…ìŠ¤íŠ¸ ì¶”ì¶œ
            for script in soup(["script", "style", "nav", "footer", "header"]):
                script.decompose()
            text = soup.get_text(separator=' ', strip=True)[:5000]
            
            # 2. ì´ë¯¸ì§€ ì¶”ì¶œ (og:image -> twitter:image -> ì—†ìœ¼ë©´ ê¸°ë³¸ê°’)
            image_url = ""
            og_image = soup.find("meta", property="og:image")
            
            if og_image and og_image.get("content"):
                image_url = og_image["content"]
            else:
                # [í•µì‹¬] ì´ë¯¸ì§€ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ì´ë¯¸ì§€ë¥¼ ë„£ì–´ë¼!
                print("âš ï¸ ì´ë¯¸ì§€ë¥¼ ëª» ì°¾ì•˜ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                image_url = DEFAULT_IMAGE
            
            return text, image_url
            
        except Exception as e:
            print(f"Crawling Error: {e}")
            return None, None
        
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
    }
    async with httpx.AsyncClient(follow_redirects=True) as client:
        try:
            response = await client.get(url, headers=headers, timeout=10.0)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            
            for script in soup(["script", "style", "nav", "footer", "header"]):
                script.decompose()
            text = soup.get_text(separator=' ', strip=True)[:5000]
            
            image_url = ""
            og_image = soup.find("meta", property="og:image")
            if og_image:
                image_url = og_image["content"]
            
            return text, image_url
        except Exception as e:
            print(f"Crawling Error: {e}")
            return None, None

# [Update] ìºì‹œ ì €ì¥ì„ ìœ„í•´ êµ¬ì¡° ë³€ê²½
async def gemini_stream_generator(text, image_url, url_key):
    full_response = "" # ì „ì²´ ë‚´ìš©ì„ ëª¨ì„ ë³€ìˆ˜
    
    # 1. ì´ë¯¸ì§€ URL ë¨¼ì € ì „ì†¡
    if image_url:
        img_msg = f"IMAGE_URL::{image_url}::END\n"
        yield img_msg
        full_response += img_msg
    
    prompt = f"""
    [System Instruction]
    ë„ˆëŠ” 'Liquid UI'ì˜ AI ì—”ì§„ì´ë‹¤.
    ì‚¬ìš©ìê°€ ì œê³µí•œ ì›¹í˜ì´ì§€ í…ìŠ¤íŠ¸ë¥¼ ëª¨ë°”ì¼ í™˜ê²½ì— ë§ì¶° [3ì¤„ ìš”ì•½]í•´ë¼.
    í•µì‹¬ ì •ë³´ë§Œ ë‚¨ê¸°ê³ , ë§íˆ¬ëŠ” ê±´ì¡°í•˜ê³  ëª…í™•í•˜ê²Œ í•œêµ­ì–´ë¡œ ì‘ì„±í•´ë¼.
    
    [Input Text]
    {text} 
    """

    try:
        response = await model.generate_content_async(prompt, stream=True)
        async for chunk in response:
            if chunk.text:
                yield chunk.text
                full_response += chunk.text # ë§í•˜ëŠ” ì¡±ì¡± ëª¨ì€ë‹¤
        
        # [NEW] 2. ë‹¤ ë§í–ˆìœ¼ë©´ ìºì‹œì— ì €ì¥ (URLì„ í‚¤ê°’ìœ¼ë¡œ)
        summary_cache.put(url_key, full_response)
        print(f"ğŸ’¾ Cached Saved for: {url_key}")

    except Exception as e:
        yield f"Error: {str(e)}"

@app.post("/analyze")
async def analyze_page(request: AnalyzeRequest):
    print(f"Request received for: {request.url}")
    
    # [NEW] 3. ìºì‹œ í™•ì¸ (ê¸°ì–µ ì†ì— ìˆë‚˜?)
    cached_data = summary_cache.get(request.url)
    if cached_data:
        print(f"âš¡ Cache Hit! (ì´ˆê³ ì† ì‘ë‹µ): {request.url}")
        # ì €ì¥ëœ ê±° ë°”ë¡œ ë±‰ì–´ì£¼ëŠ” í•¨ìˆ˜
        async def cached_stream():
            yield cached_data
        return StreamingResponse(cached_stream(), media_type="text/event-stream")

    # ìºì‹œì— ì—†ìœ¼ë©´? -> í¬ë¡¤ë§ ì‹œì‘
    target_text = request.text_content
    image_url = ""

    if not target_text:
        print("Text missing. Server will crawl...")
        fetched_text, fetched_image = await fetch_page_content(request.url)
        if fetched_text:
            target_text = fetched_text
            image_url = fetched_image
        else:
            return StreamingResponse(iter(["Error: í¬ë¡¤ë§ ì‹¤íŒ¨"]), media_type="text/event-stream")
    
    # AI ìƒì„± ì‹œì‘ (urlë„ ê°™ì´ ë„˜ê²¨ì„œ ë‚˜ì¤‘ì— ì €ì¥í•˜ê²Œ í•¨)
    return StreamingResponse(
        gemini_stream_generator(target_text, image_url, request.url), 
        media_type="text/event-stream"
    )

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)